[
  {
    "id": "audio-steganography",
    "title": "Encoding a Secret Message in Sound Waves",
    "description": "Using Fourier transform (frequency domain coefficient embedding) to hide a secret message in audio files",
    "url": "https://github.com/vivienhenz24/phys_15C_lab_proj",
    "date": "October 2025",
    "content": "\nA Python implementation of audio steganography using Fourier transforms to embed secret messages in sound waves by manipulating frequency domain coefficients.\n\n[View on GitHub →](https://github.com/vivienhenz24/phys_15C_lab_proj)\n\n",
    "htmlContent": "<p>A Python implementation of audio steganography using Fourier transforms to embed secret messages in sound waves by manipulating frequency domain coefficients.</p>\n<p><a href=\"https://github.com/vivienhenz24/phys_15C_lab_proj\">View on GitHub →</a></p>\n",
    "originalDate": "2025-10-19T04:00:00.000Z"
  },
  {
    "id": "luxembourgish-tts",
    "title": "Luxembourgish text-to-speech",
    "description": "A text-to-speech system for the Luxembourgish language, making digital content accessible to Luxembourgish speakers.",
    "url": "https://neiom.io",
    "date": "October 2025",
    "content": "\nAfter having built a fair amount of transformers, I'm inclined to say that the direction in which AI is heading is completely wrong.\n\nTransformer-based models don't learn, they just overfit their pretraining data. We got away with it by scaling compute and using insane amounts of data, but now we're running out of the latter. The consequence is that languages like Luxembourgish, which fall out of the data distribution, still aren't supported by the most advanced TTS and ASR models.\n\nSo, since I'm not held back by my X $billion investments in compute infrastructure, I want to build a model which is capable of handling out-of-scope data by shifting the emphasis away from scaling pre-training data and towards continuous learning. I'm starting with Luxembourgish because that's what I know best!\n\nFor my first iteration all I did was fork the open source fish-speech TTS model and wrote my own training loop to fine tune it on 32000 recorded Luxembourgish samples. It took 5h on an RTX5090 (~10$). \n\n[Visit neiom.io →](https://neiom.io)\n",
    "htmlContent": "<p>After having built a fair amount of transformers, I'm inclined to say that the direction in which AI is heading is completely wrong.</p>\n<p>Transformer-based models don't learn, they just overfit their pretraining data. We got away with it by scaling compute and using insane amounts of data, but now we're running out of the latter. The consequence is that languages like Luxembourgish, which fall out of the data distribution, still aren't supported by the most advanced TTS and ASR models.</p>\n<p>So, since I'm not held back by my X $billion investments in compute infrastructure, I want to build a model which is capable of handling out-of-scope data by shifting the emphasis away from scaling pre-training data and towards continuous learning. I'm starting with Luxembourgish because that's what I know best!</p>\n<p>For my first iteration all I did was fork the open source fish-speech TTS model and wrote my own training loop to fine tune it on 32000 recorded Luxembourgish samples. It took 5h on an RTX5090 (~10$).</p>\n<p><a href=\"https://neiom.io\">Visit neiom.io →</a></p>\n",
    "originalDate": "2025-10-15T04:00:00.000Z"
  },
  {
    "id": "transformer-candle",
    "title": "Building a transformer from scratch",
    "description": "A complete implementation of a transformer neural network architecture built entirely from scratch using Rust and Candle. Features RoPE (Rotary Position Embedding) and BBPE (Byte-level BPE tokenization), with full attention mechanisms, positional encoding, and multi-head attention.",
    "url": "https://github.com/vivienhenz24/transformer_candle",
    "date": "September 2025",
    "content": "\nThere's no rational reason for building a transformer in Rust—except that it's fun. It's like the guy that built a transformer using minecraft redstone, except lamer.\n\n\n[View on GitHub →](https://github.com/vivienhenz24/transformer_candle)\n",
    "htmlContent": "<p>There's no rational reason for building a transformer in Rust—except that it's fun. It's like the guy that built a transformer using minecraft redstone, except lamer.</p>\n<p><a href=\"https://github.com/vivienhenz24/transformer_candle\">View on GitHub →</a></p>\n",
    "originalDate": "2025-09-01T04:00:00.000Z"
  },
  {
    "id": "parsed",
    "title": "Parsed",
    "description": "My first big coding project, a mobile app that gives you the five most important news headlines of the day, articles about these headlines written by newspapers from all over the political spectrum. And a bias analysis for each news item presented to you.",
    "url": "https://apps.apple.com/tn/app/parsed/id6743483636",
    "date": "March 2025",
    "content": "\nParsed was the first mobile app I ever made. Obviously it wasn't something people wanted, but it taught me a lot and I don't regret one bit of spending two months building this thing. \n\n2500 downloads \n\n[Download on the App Store →](https://apps.apple.com/tn/app/parsed/id6743483636)\n",
    "htmlContent": "<p>Parsed was the first mobile app I ever made. Obviously it wasn't something people wanted, but it taught me a lot and I don't regret one bit of spending two months building this thing.</p>\n<p>2500 downloads</p>\n<p><a href=\"https://apps.apple.com/tn/app/parsed/id6743483636\">Download on the App Store →</a></p>\n",
    "originalDate": "2025-03-29T04:00:00.000Z"
  }
]