[
  {
    "id": "transformer-candle",
    "title": "Building a transformer from scratch",
    "description": "A complete implementation of a transformer neural network architecture built entirely from scratch using Rust and Candle. Features RoPE (Rotary Position Embedding) and BBPE (Byte-level BPE tokenization), with full attention mechanisms, positional encoding, and multi-head attention.",
    "url": "https://github.com/vivienhenz24/transformer_candle",
    "date": "September 2025",
    "content": "\nThere's no rational reason for building a transformer in Rust—except that it's fun.\n\n## Why Rust?\n\nWhen I started this project, I was curious about the performance characteristics of neural networks implemented in systems programming languages. While Python dominates the ML ecosystem, I wanted to explore what was possible with Rust's memory safety and performance guarantees.\n\n## Architecture Overview\n\nThe transformer implements:\n\n- **Multi-head attention** with configurable heads\n- **RoPE (Rotary Position Embedding)** for positional encoding\n- **BBPE (Byte-level BPE)** tokenization\n- **Layer normalization** and residual connections\n- **Feed-forward networks** with configurable dimensions\n\n## Key Challenges\n\nThe most interesting challenge was implementing attention mechanisms efficiently in Rust. Unlike Python's NumPy, I had to be explicit about memory layouts and tensor operations.\n\nAnother fascinating aspect was implementing RoPE from scratch. The mathematical formulation is elegant, but translating it to efficient Rust code required careful attention to the rotation matrices and their application.\n\n## Performance\n\nWhile not as optimized as production libraries like PyTorch, the Rust implementation shows promising performance characteristics, especially for inference tasks where memory safety and predictable performance matter.\n\n## What I Learned\n\nThis project taught me that building neural networks from scratch is not just about understanding the math—it's about understanding how the math translates to efficient computation. Every operation, from matrix multiplications to attention computations, has performance implications that become apparent when you implement them yourself.\n\nThe experience also reinforced my appreciation for the abstractions that high-level ML frameworks provide. What takes a few lines in PyTorch can require hundreds of lines of careful Rust code.\n\n## Future Work\n\nI'm considering adding:\n- GPU acceleration with CUDA bindings\n- More sophisticated optimization techniques\n- Support for different model architectures\n- A more user-friendly API\n\n[View on GitHub →](https://github.com/vivienhenz24/transformer_candle)\n",
    "htmlContent": "<p>There's no rational reason for building a transformer in Rust—except that it's fun.</p>\n<h2>Why Rust?</h2>\n<p>When I started this project, I was curious about the performance characteristics of neural networks implemented in systems programming languages. While Python dominates the ML ecosystem, I wanted to explore what was possible with Rust's memory safety and performance guarantees.</p>\n<h2>Architecture Overview</h2>\n<p>The transformer implements:</p>\n<ul>\n<li><strong>Multi-head attention</strong> with configurable heads</li>\n<li><strong>RoPE (Rotary Position Embedding)</strong> for positional encoding</li>\n<li><strong>BBPE (Byte-level BPE)</strong> tokenization</li>\n<li><strong>Layer normalization</strong> and residual connections</li>\n<li><strong>Feed-forward networks</strong> with configurable dimensions</li>\n</ul>\n<h2>Key Challenges</h2>\n<p>The most interesting challenge was implementing attention mechanisms efficiently in Rust. Unlike Python's NumPy, I had to be explicit about memory layouts and tensor operations.</p>\n<p>Another fascinating aspect was implementing RoPE from scratch. The mathematical formulation is elegant, but translating it to efficient Rust code required careful attention to the rotation matrices and their application.</p>\n<h2>Performance</h2>\n<p>While not as optimized as production libraries like PyTorch, the Rust implementation shows promising performance characteristics, especially for inference tasks where memory safety and predictable performance matter.</p>\n<h2>What I Learned</h2>\n<p>This project taught me that building neural networks from scratch is not just about understanding the math—it's about understanding how the math translates to efficient computation. Every operation, from matrix multiplications to attention computations, has performance implications that become apparent when you implement them yourself.</p>\n<p>The experience also reinforced my appreciation for the abstractions that high-level ML frameworks provide. What takes a few lines in PyTorch can require hundreds of lines of careful Rust code.</p>\n<h2>Future Work</h2>\n<p>I'm considering adding:</p>\n<ul>\n<li>GPU acceleration with CUDA bindings</li>\n<li>More sophisticated optimization techniques</li>\n<li>Support for different model architectures</li>\n<li>A more user-friendly API</li>\n</ul>\n<p><a href=\"https://github.com/vivienhenz24/transformer_candle\">View on GitHub →</a></p>\n",
    "originalDate": "2025-09-01T04:00:00.000Z"
  },
  {
    "id": "parsed",
    "title": "Parsed",
    "description": "My first big coding project, a mobile app that gives you the five most important news headlines of the day, articles about these headlines written by newspapers from all over the political spectrum. And a bias analysis for each news item presented to you.",
    "url": "https://apps.apple.com/tn/app/parsed/id6743483636",
    "date": "March 2025",
    "content": "\n2500+ downloads as of today :) Available on the App Store.\n\n## The Problem\n\nIn our hyper-connected world, we're drowning in information but starving for understanding. News comes at us from every direction, but it's hard to know what really matters and how to think about it critically.\n\n## The Solution\n\nParsed curates the five most important news stories of the day and presents them alongside articles from newspapers across the political spectrum. Each story includes a bias analysis to help you understand different perspectives.\n\n## Technical Implementation\n\nThe app was built using React Native, allowing for cross-platform deployment on both iOS and Android. The backend processes news from various sources and uses natural language processing to:\n\n- Identify the most significant stories\n- Categorize sources by political leaning\n- Generate bias analysis for each perspective\n\n## Key Features\n\n- **Daily curation**: Five stories that actually matter\n- **Multi-perspective coverage**: See how different outlets cover the same story\n- **Bias analysis**: Understand the political leanings of each source\n- **Clean, focused interface**: No endless scrolling or clickbait\n\n## What I Learned\n\nThis project taught me that good software isn't just about clean code—it's about solving real problems for real people. The technical challenges were interesting, but the user experience challenges were even more rewarding.\n\nI also learned the importance of iteration. The first version was completely different from what users actually wanted. Listening to feedback and being willing to pivot was crucial.\n\n## Impact\n\nSeeing 2500+ people download and use something I built was incredibly rewarding. It's one thing to build something for yourself, but quite another to build something that other people find valuable.\n\n[Download on the App Store →](https://apps.apple.com/tn/app/parsed/id6743483636)\n",
    "htmlContent": "<p>2500+ downloads as of today :) Available on the App Store.</p>\n<h2>The Problem</h2>\n<p>In our hyper-connected world, we're drowning in information but starving for understanding. News comes at us from every direction, but it's hard to know what really matters and how to think about it critically.</p>\n<h2>The Solution</h2>\n<p>Parsed curates the five most important news stories of the day and presents them alongside articles from newspapers across the political spectrum. Each story includes a bias analysis to help you understand different perspectives.</p>\n<h2>Technical Implementation</h2>\n<p>The app was built using React Native, allowing for cross-platform deployment on both iOS and Android. The backend processes news from various sources and uses natural language processing to:</p>\n<ul>\n<li>Identify the most significant stories</li>\n<li>Categorize sources by political leaning</li>\n<li>Generate bias analysis for each perspective</li>\n</ul>\n<h2>Key Features</h2>\n<ul>\n<li><strong>Daily curation</strong>: Five stories that actually matter</li>\n<li><strong>Multi-perspective coverage</strong>: See how different outlets cover the same story</li>\n<li><strong>Bias analysis</strong>: Understand the political leanings of each source</li>\n<li><strong>Clean, focused interface</strong>: No endless scrolling or clickbait</li>\n</ul>\n<h2>What I Learned</h2>\n<p>This project taught me that good software isn't just about clean code—it's about solving real problems for real people. The technical challenges were interesting, but the user experience challenges were even more rewarding.</p>\n<p>I also learned the importance of iteration. The first version was completely different from what users actually wanted. Listening to feedback and being willing to pivot was crucial.</p>\n<h2>Impact</h2>\n<p>Seeing 2500+ people download and use something I built was incredibly rewarding. It's one thing to build something for yourself, but quite another to build something that other people find valuable.</p>\n<p><a href=\"https://apps.apple.com/tn/app/parsed/id6743483636\">Download on the App Store →</a></p>\n",
    "originalDate": "2025-03-29T04:00:00.000Z"
  }
]